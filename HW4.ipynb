{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import torch#use: pip install torch, then restart the kernel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms#pip install torchvision\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boats(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None, gt_json_path=''):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.gt_json_path = gt_json_path\n",
    "        self.labels = json.load(open(gt_json_path, 'r'))\n",
    "        self.image_list = sorted(os.listdir(root_dir))\n",
    "        self.image_ids = dict(enumerate(self.image_list, start=0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(idx)\n",
    "        img_name = self.image_ids[idx]\n",
    "        label = self.labels[img_name]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        sample = (img, label)\n",
    "        return sample\n",
    "\n",
    "    def load_image(self, image_index):\n",
    "        image_name = self.image_ids[image_index]\n",
    "        path = os.path.join(self.root_dir, image_name)\n",
    "        img = imread(path)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Net(nn.Module):#TODO 9) #creat your own NN architecture\n",
    "    # Define neutral network\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 192 * 108, 1)\n",
    "        '''\n",
    "        TODO Question 1: \n",
    "        fc1 defines the first fully connected layer (linear layer) with input size 3 * 192 * 108 and 1 output lebel. \n",
    "        3 means 3 channels, 192 * 108 is the size of the img.\n",
    "        '''\n",
    "\n",
    "    # Specify how data will pass through your model, this function will pass the data into the NN\n",
    "    def forward(self, x):\n",
    "        #\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        '''\n",
    "        TODO Question 2: \n",
    "        x represents our multi-dimension input data (tensor), start_dim represents the dimension to start. \n",
    "        This line of code flatten the multi-dimensional input tensor into 2D shape (i.e. (batch_size, features)) starting from dimension 1.\n",
    "        The 'features' dimension is used to represent all the input data for each example in the batch, so that it can be passed into the fc layer\n",
    "        '''\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        '''\n",
    "        TODO Question 3: \n",
    "        this line pass the flattened data x to the fully connected layer we defined\n",
    "        '''\n",
    "        \n",
    "        output = torch.sigmoid(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, criterion, epoch,dry_run):\n",
    "    \"\"\"\n",
    "    Train a network\n",
    "    You can find example code here: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        '''\n",
    "        TODO Question 4:\n",
    "        this line clears out the gradients from the previous batch before computing gradients for the next batch\n",
    "        '''\n",
    "        output = model(data)\n",
    "        loss = criterion(output, torch.unsqueeze(target, 1))\n",
    "        '''\n",
    "        TODO Question 5:\n",
    "        this line computes the prediction error (loss) based on the predicted output and the target label.\n",
    "        torch.unsqueeze(target, 1) is used to accommodate the format if necessary\n",
    "        '''\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device).float()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, torch.unsqueeze(target, 1)).item()  # sum up batch loss\n",
    "            pred = torch.round(output)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/courses/CS5330.202450/data/Boat-MNIST/boat_mnist_labels_trainval.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m\n\u001b[0;32m    100\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(model, dummy_input, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mship_example.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_names\u001b[38;5;241m=\u001b[39minput_names, output_names\u001b[38;5;241m=\u001b[39moutput_names)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[6], line 62\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Create train and test set\u001b[39;00m\n\u001b[0;32m     61\u001b[0m path_to_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/courses/CS5330.202450/data/Boat-MNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;66;03m#gobal path to the data on Discovery \u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m train_set \u001b[38;5;241m=\u001b[39m Boats(root_dir\u001b[38;5;241m=\u001b[39mpath_to_dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m     63\u001b[0m                   gt_json_path\u001b[38;5;241m=\u001b[39mpath_to_dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/boat_mnist_labels_trainval.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m val_set \u001b[38;5;241m=\u001b[39m Boats(root_dir\u001b[38;5;241m=\u001b[39mpath_to_dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/val\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[0;32m     65\u001b[0m                 gt_json_path\u001b[38;5;241m=\u001b[39mpath_to_dataset \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/boat_mnist_labels_trainval.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mBoats.__init__\u001b[1;34m(self, root_dir, transform, gt_json_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_json_path \u001b[38;5;241m=\u001b[39m gt_json_path\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(gt_json_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(root_dir))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_list, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/courses/CS5330.202450/data/Boat-MNIST/boat_mnist_labels_trainval.json'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Training settings #you can mess around with change these values!\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 1\n",
    "    learning_rate = 0.001\n",
    "    no_cuda = False #If you using course cpu leave False, if you are using GPU set true\n",
    "    dry_run = False\n",
    "    seed = random.randint(1,1000)#random seed. Set to constant if you want to train on the same data\n",
    "    log_interval = 10#how many batches to wait before logging training status\n",
    "    save_model = False \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #This is used if you want to run it as a script file.\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Ship Detection')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 0.1)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    #torch.manual_seed(args.seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    use_cuda = no_cuda\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #train_kwargs = {'batch_size': args.batch_size}\n",
    "    #val_kwargs = {'batch_size': args.test_batch_size}\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    val_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        val_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    # Create transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # This normalization is used on the test server\n",
    "        transforms.Normalize([0.2404, 0.2967, 0.3563], [0.0547, 0.0527, 0.0477])\n",
    "        ])\n",
    "\n",
    "    # Create train and test set\n",
    "    path_to_dataset = \"/courses/CS5330.202450/data/Boat-MNIST\"#gobal path to the data on Discovery \n",
    "    train_set = Boats(root_dir=path_to_dataset + \"/train\", transform=transform,\n",
    "                      gt_json_path=path_to_dataset + \"/boat_mnist_labels_trainval.json\")\n",
    "    val_set = Boats(root_dir=path_to_dataset + \"/val\", transform=transform,\n",
    "                    gt_json_path=path_to_dataset +\"/boat_mnist_labels_trainval.json\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(val_set, **val_kwargs)\n",
    "\n",
    "    # Create network, optimizer and loss\n",
    "    model = Net().to(device)\n",
    "    '''\n",
    "    TODO Question 6\n",
    "    this line creates a model by calling Net() class and move the model to the specified device: cpu or gpu\n",
    "    '''\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    '''\n",
    "    TODO Question 7\n",
    "    this line initialize a Stochastic gradient descent optimizer (as the chosen optimization function) with specified model paramters and learning rate.\n",
    "    the model.parameters() function call retrieves all the parameters of the neural network model that will be optimized during training (the weights and biases of the model)\n",
    "    \n",
    "    '''\n",
    "    criterion = nn.MSELoss()\n",
    "    '''\n",
    "    TODO Question 8\n",
    "    this line specifies that the MSE (mean squared error) is used as the criterion to measure loss, by specifying MSELoss function\n",
    "    '''\n",
    "\n",
    "    # Train and validate\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(log_interval, model, device, train_loader, optimizer, criterion, epoch, dry_run)\n",
    "        acc = test(model, device, test_loader, criterion)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Best accuracy (val): {best_acc}\")\n",
    "\n",
    "    #if args.save_model:\n",
    "    #    torch.save(model.state_dict(), \"model.pth\")\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "    \n",
    "    # --- Do not touch -----\n",
    "    # Save model as onnx file\n",
    "    dummy_input = torch.randn(1, 3, 108, 192, device=device)\n",
    "    input_names = [\"img_1\"]\n",
    "    output_names = [\"output1\"]\n",
    "    torch.onnx.export(model, dummy_input, \"ship_example.onnx\", input_names=input_names, output_names=output_names)\n",
    "    # ----------------------\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
